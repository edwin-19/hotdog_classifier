{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug.imgaug\n",
    "import focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_list, labels, batch_size=16, input_dim=(512, 512), n_classes=2, shuffle=True, augment=False):\n",
    "        self.input_dim = input_dim\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_list) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        \n",
    "        # Find matching image list according to batch size\n",
    "        list_id_temps = [self.image_list[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        x, y = self.data_generator(list_id_temps)\n",
    "        if self.augment:\n",
    "            x = self.augmentor(x)\n",
    "        \n",
    "#         x = np.array([img / 255. for img in x])\n",
    "        return x, y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_list))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def data_generator(self, list_id_temp):\n",
    "        # Initialize\n",
    "        x = np.empty((self.batch_size, self.input_dim[0], self.input_dim[1], 3))\n",
    "        y = np.empty((self.batch_size), dtype=np.int)\n",
    "        \n",
    "        # Generate data\n",
    "        for index, ids in enumerate(list_id_temp):\n",
    "            img = cv2.imread(ids)\n",
    "            img = cv2.resize(img, self.input_dim)\n",
    "            img = img / 255.\n",
    "            \n",
    "            label = self.labels[os.path.basename(os.path.dirname(ids))]\n",
    "            \n",
    "            x[index, ] = img\n",
    "            y[index, ] = label\n",
    "            \n",
    "        return x, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "    \n",
    "    def augmentor(self, images):\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Sometimes(0.5,\n",
    "                iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "            ),\n",
    "            iaa.Affine(\n",
    "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                rotate=(-25, 25)\n",
    "            )\n",
    "        ], random_order=True)\n",
    "        \n",
    "        return seq(images=images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "input_size = (128, 128, 3)\n",
    "learning_rate = 1e-4\n",
    "batch_size = 16\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tensorflow-Keras Implementation of Mish\"\"\"\n",
    "\n",
    "## Import Necessary Modules\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "get_custom_objects().update({'Mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 61, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,718,402\n",
      "Trainable params: 1,718,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=input_size))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3)))\n",
    "model.add(tf.keras.layers.Activation('Mish'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3)))\n",
    "model.add(tf.keras.layers.Activation('Mish'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3)))\n",
    "model.add(tf.keras.layers.Activation('Mish'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('Mish'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'hot_dog': 1,\n",
    "    'not_hot_dog': 0\n",
    "}\n",
    "\n",
    "train_data = glob.glob('data/train/**/*.jpg', recursive=True)\n",
    "test_data = glob.glob('data/test/**/*.jpg', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGen(train_data, labels, batch_size=batch_size, input_dim=input_size[:2], augment=False)\n",
    "test_gen = ImageDataGen(test_data, labels, batch_size=batch_size, input_dim=input_size[:2], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', mode='max', patience=10, verbose=1, min_delta=0.001\n",
    ")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', mode='max', patience=5, verbose=1, min_delta=0.001\n",
    ")\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'weights/hotdog_classifier.h5', monitor='val_accuracy', mode='max', save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 12s 375ms/step - loss: 0.6954 - accuracy: 0.5060 - val_loss: 0.6931 - val_accuracy: 0.5202 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 12s 381ms/step - loss: 0.6627 - accuracy: 0.5968 - val_loss: 0.6855 - val_accuracy: 0.5343 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 11s 366ms/step - loss: 0.6482 - accuracy: 0.6290 - val_loss: 0.7076 - val_accuracy: 0.5323 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 12s 376ms/step - loss: 0.6453 - accuracy: 0.6230 - val_loss: 0.6825 - val_accuracy: 0.5806 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 12s 390ms/step - loss: 0.6035 - accuracy: 0.7036 - val_loss: 0.6806 - val_accuracy: 0.5645 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 15s 471ms/step - loss: 0.5699 - accuracy: 0.7319 - val_loss: 0.6944 - val_accuracy: 0.5625 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 18s 585ms/step - loss: 0.5545 - accuracy: 0.7379 - val_loss: 0.7042 - val_accuracy: 0.5726 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 12s 397ms/step - loss: 0.5193 - accuracy: 0.7540 - val_loss: 0.7102 - val_accuracy: 0.5746 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 13s 412ms/step - loss: 0.5071 - accuracy: 0.7742 - val_loss: 0.7246 - val_accuracy: 0.5887 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 12s 396ms/step - loss: 0.4647 - accuracy: 0.8024 - val_loss: 0.7493 - val_accuracy: 0.5948 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 14s 437ms/step - loss: 0.4328 - accuracy: 0.8024 - val_loss: 0.7834 - val_accuracy: 0.5645 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 12s 395ms/step - loss: 0.4356 - accuracy: 0.7923 - val_loss: 0.7641 - val_accuracy: 0.5968 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 13s 413ms/step - loss: 0.3674 - accuracy: 0.8448 - val_loss: 0.8286 - val_accuracy: 0.5887 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 12s 400ms/step - loss: 0.3414 - accuracy: 0.8367 - val_loss: 0.8418 - val_accuracy: 0.5927 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 11s 366ms/step - loss: 0.3294 - accuracy: 0.8508 - val_loss: 0.9134 - val_accuracy: 0.5948 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 12s 373ms/step - loss: 0.2881 - accuracy: 0.9073 - val_loss: 0.9261 - val_accuracy: 0.6008 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 12s 381ms/step - loss: 0.2565 - accuracy: 0.8952 - val_loss: 0.9929 - val_accuracy: 0.6089 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 12s 377ms/step - loss: 0.2347 - accuracy: 0.9113 - val_loss: 0.9884 - val_accuracy: 0.5766 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 13s 406ms/step - loss: 0.2074 - accuracy: 0.9294 - val_loss: 1.0407 - val_accuracy: 0.5847 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 12s 388ms/step - loss: 0.1830 - accuracy: 0.9375 - val_loss: 1.1271 - val_accuracy: 0.5786 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 12s 396ms/step - loss: 0.1785 - accuracy: 0.9375 - val_loss: 1.1082 - val_accuracy: 0.5887 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=epoch,\n",
    "    validation_data=test_gen,\n",
    "    validation_steps=len(test_data) // batch_size,\n",
    "    callbacks=[reduce_lr, early_stopping, model_ckpt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('weights/hotdog_classifier.h5', compile=False, custom_objects={'mish': tf.keras.layers.Activation('Mish')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "for dat in test_data:\n",
    "    img = cv2.imread(dat)\n",
    "    img = cv2.resize(img, input_size[:2])\n",
    "    img = img / 255.\n",
    "    y_pred.append(img)\n",
    "    \n",
    "    y_true.append(labels[os.path.basename(os.path.dirname(dat))])\n",
    "\n",
    "y_pred = model.predict(np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y_pred = []\n",
    "for pred in y_pred:\n",
    "    true_y_pred.append(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_true, true_y_pred)\n",
    "conf_mat = confusion_matrix(y_true, true_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Negative', 'Positive']\n",
    "group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "\n",
    "group_percentage = [\n",
    "    '{0:.2%}'.format(value) for value in conf_mat.ravel() / conf_mat.sum()\n",
    "]\n",
    "\n",
    "cm_labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentage)]\n",
    "\n",
    "cm_labels = np.array(cm_labels).reshape(2, 2)\n",
    "\n",
    "plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
    "sns.heatmap(\n",
    "    conf_mat, annot=cm_labels, fmt='',\n",
    "    xticklabels=categories, yticklabels=categories\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visually Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in test_data[:9]:\n",
    "    img = cv2.imread(image_path)\n",
    "    img_copy = cv2.resize(img, input_size[:2])\n",
    "    img_copy = img_copy / 255.\n",
    "    \n",
    "    pred = model.predict(np.expand_dims(img_copy, axis=0))\n",
    "    \n",
    "    plt.title(np.argmax(pred))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/superceed1/anaconda3/envs/py36tf21/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: weights/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, 'weights/saved_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n"
     ]
    }
   ],
   "source": [
    "loaded = tf.saved_model.load('weights/saved_model/')\n",
    "print(list(loaded.signatures.keys()))  # [\"serving_default\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
